---
title: "To split or not to split that is the question: From cross validation to debiased machine learning"
author: "Morgane Austern"
date: 2023-01-13
categories: ["McGill Statistics Seminar"]
tags: ["2023 Winter"]
---

#### Date: 2023-01-13
#### Time: 15:30-16:30 (Montreal time)

#### [https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09](https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09)
#### Meeting ID: 834 3668 6293
#### Passcode: 12345



## Abstract:

Data splitting is an ubiquitous method in statistics with examples ranging from cross validation to cross-fitting. However, despite its prevalence, theoretical guidance regarding its use is still lacking. In this talk we will explore two examples and establish an asymptotic theory for it. In the first part of this talk, we study the cross-validation method, a ubiquitous method for risk estimation, and establish its asymptotic properties for a large class of models and with an arbitrary number of folds. Under stability conditions, we establish a central limit theorem and Berry-Esseen bounds for the cross-validated risk, which enable us to compute asymptotically accurate confidence intervals. Using our results, we study the statistical speed-up offered by cross validation compared to a train-test split procedure. We reveal some surprising behavior of the cross-validated risk and establish the statistically optimal choice for the number of folds. In the second part of this talk, we study the role of cross fitting in the generalized method of moments with moments that also depend on some auxiliary functions. Recent lines of work show how one can use generic machine learning estimators for these auxiliary problems, while maintaining asymptotic normality and root-n consistency of the target parameter of interest. The literature typically requires that these auxiliary problems are fitted on a separate sample or in a cross-fitting manner. We show that when these auxiliary estimation algorithms satisfy natural leave-one-out stability properties, then sample splitting is not required. This allows for sample re-use, which can be beneficial in moderately sized sample regimes.

## Speaker

Morgane Austern is an Assistant Professor of Statistics at Harvard University. She obtained her PhD in statistics from Columbia University, working with Peter Orbanz and Arian Maleki on limit theorems for dependent and structured data.  After that, she was a postdoctoral researcher at Microsoft Research New England. Broadly, she interested in developing probability tools for modern machine learning and in establishing the properties of learning algorithms in structured and dependent data contexts.  Her current work is motivated by generalization and concentration bounds, stable matching problems and random matrix theory.

